{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37e4e83",
   "metadata": {},
   "source": [
    "## 1. Import and Intiallize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4a262",
   "metadata": {},
   "source": [
    "# Import\n",
    "Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ce3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, itertools\n",
    "from glob import glob\n",
    "import torch, lpips\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lpips_fn = lpips.LPIPS(net='alex').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dfbcb",
   "metadata": {},
   "source": [
    "# Path and Directories\n",
    "Define file paths for training and testing CT and MRI datasets and create an output directory to save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ct_dir = \"/path/to/data/trainA\"\n",
    "train_mri_dir = \"/path/to/data/trainB\"\n",
    "test_ct_dir = \"/path/to/data/testA\"\n",
    "test_mri_dir = \"/path/to/data/testB\"\n",
    "out_dir = \"./cyclegan_v3\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ad14a",
   "metadata": {},
   "source": [
    "# Hyperparameter\n",
    "Define hyperparameter for training Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 1\n",
    "learningRate = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "total_epochs = 1000\n",
    "lambda_cycle = 10.0\n",
    "lambda_id = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67daded3",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Load training and test Set by \"randomly\" pairing MRI and CT image for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnpairedDataset(Dataset):\n",
    "    def __init__(self, ct_dir, mri_dir, tf):\n",
    "        self.cts  = sorted(glob(f\"{ct_dir}/*.*\"))\n",
    "        self.mris = sorted(glob(f\"{mri_dir}/*.*\"))\n",
    "        self.tf   = tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.cts), len(self.mris))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ct  = Image.open(self.cts[idx % len(self.cts)]).convert(\"RGB\")\n",
    "        img_mri = Image.open(random.choice(self.mris)).convert(\"RGB\")\n",
    "        return self.tf(img_ct), self.tf(img_mri)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3),\n",
    "])\n",
    "\n",
    "train_ds = UnpairedDataset(train_ct_dir, train_mri_dir, transform)\n",
    "test_ds = UnpairedDataset(test_ct_dir,  test_mri_dir,  transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5de087",
   "metadata": {},
   "source": [
    "## 2. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910747b",
   "metadata": {},
   "source": [
    "# ResNet Generator\n",
    "Implement ResNet based generator as its residual connections help preserve image structure and stabilize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 0), nn.InstanceNorm2d(dim), nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 0), nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=3, n_res=9):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_ch, 64, 7, 1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        c = 64\n",
    "        # Downsampling (Encoder)\n",
    "        for _ in range(2):\n",
    "            layers += [nn.Conv2d(c, c*2, 3, 2, 1), nn.InstanceNorm2d(c*2), nn.ReLU(True)]\n",
    "            c *= 2\n",
    "        # Residual\n",
    "        for _ in range(n_res):\n",
    "            layers += [ResnetBlock(c)]\n",
    "        # Upsampling (Decoder)\n",
    "        for _ in range(2):\n",
    "            layers += [nn.ConvTranspose2d(c, c//2, 3, 2, 1, output_padding=1),\n",
    "                       nn.InstanceNorm2d(c//2), nn.ReLU(True)]\n",
    "            c //= 2\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, out_ch, 7, 1),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3758ff",
   "metadata": {},
   "source": [
    "# PatchGAN Discriminator\n",
    "Implement PatchGAN based discriminator to evaluate generated output as it evaluate image in patches instead of the whole, focusing on local high-frequency features to better detect realistic textures and details for improved adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8688e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=3, feats=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, feats[0], 4, 2, 1), nn.LeakyReLU(0.2, True)]\n",
    "        prev = feats[0]\n",
    "        for f in feats[1:]:\n",
    "            layers += [nn.Conv2d(prev, f, 4, 2, 1, bias=False),\n",
    "                       nn.InstanceNorm2d(f), nn.LeakyReLU(0.2, True)]\n",
    "            prev = f\n",
    "        layers += [nn.Conv2d(prev, 1, 4, 1, 1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99d8c0",
   "metadata": {},
   "source": [
    "## 3. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90315c02",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB = ResNetGenerator().to(device)  # CT to MRI\n",
    "G_BA = ResNetGenerator().to(device)  # MRI to CT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af5d2b",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_A  = PatchGANDiscriminator().to(device) # CT\n",
    "D_B  = PatchGANDiscriminator().to(device) # MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05909f4",
   "metadata": {},
   "source": [
    "# Weight\n",
    "Initialize the weights of convolutional and instance normalization layers with a normal distribution to stabilize training and improve model convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.InstanceNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "for net in [G_AB, G_BA, D_A, D_B]:\n",
    "    init_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc323b",
   "metadata": {},
   "source": [
    "## 4. Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e89649",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "Adversarial Loss(MSE): Train generators to fool discriminators  \n",
    "Cycle Consistency Loss(MAE): Ensure input images can be accurately reconstructed from generated output to maintain cyclic integrity  \n",
    "Identity Loss(MAE): Ensure generator preserves color and content when the input already belongs to the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loss = nn.MSELoss().to(device)\n",
    "cycle_loss = nn.L1Loss().to(device)\n",
    "identity_loss = nn.L1Loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c15ed0",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "Adam optimizer for generators and discriminators, and create fake image buffers for training stability, and define helper for real/fake target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_G   = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
    "                     lr=learningRate, betas=(0.5, 0.999))\n",
    "opt_D_A = optim.Adam(D_A.parameters(), lr=learningRate, betas=(0.5, 0.999))\n",
    "opt_D_B = optim.Adam(D_B.parameters(), lr=learningRate, betas=(0.5, 0.999))\n",
    "\n",
    "fake_A_buffer, fake_B_buffer = [], []\n",
    "\n",
    "def target_tensor(pred, real):\n",
    "    return torch.ones_like(pred) if real else torch.zeros_like(pred)\n",
    "\n",
    "losses = {'adv': [], 'cycle': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23021038",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee888c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, total_epochs+1):\n",
    "    sum_adv, sum_cycle = 0.0, 0.0\n",
    "    for real_CT, real_MRI in train_loader:\n",
    "        real_CT, real_MRI = real_CT.to(device), real_MRI.to(device)\n",
    "\n",
    "        # Generators\n",
    "        opt_G.zero_grad()\n",
    "        # identity\n",
    "        id_CT  = G_BA(real_CT)\n",
    "        id_MRI = G_AB(real_MRI)\n",
    "        loss_id = (identity_loss(id_CT, real_CT) + identity_loss(id_MRI, real_MRI)) * lambda_id\n",
    "        # adversarial\n",
    "        fake_CT  = G_BA(real_MRI)\n",
    "        fake_MRI = G_AB(real_CT)\n",
    "        loss_GAN_BA = adv_loss(D_A(fake_CT), target_tensor(fake_CT, True))\n",
    "        loss_GAN_AB = adv_loss(D_B(fake_MRI), target_tensor(fake_MRI, True))\n",
    "        # cycle\n",
    "        rec_CT  = G_BA(fake_MRI)\n",
    "        rec_MRI = G_AB(fake_CT)\n",
    "        loss_cyc = (cycle_loss(rec_CT, real_CT) + cycle_loss(rec_MRI, real_MRI)) * lambda_cycle\n",
    "\n",
    "        loss_G = loss_id + loss_GAN_BA + loss_GAN_AB + loss_cyc\n",
    "        loss_G.backward(); opt_G.step()\n",
    "\n",
    "        sum_adv   += (loss_GAN_BA + loss_GAN_AB).item()\n",
    "        sum_cycle += loss_cyc.item()\n",
    "\n",
    "        # DiscriminatorA (CT)\n",
    "        opt_D_A.zero_grad()\n",
    "        real_loss_A = adv_loss(D_A(real_CT), target_tensor(real_CT, True))\n",
    "        fake_A      = fake_A_buffer.append(fake_CT.detach()) or fake_CT.detach()\n",
    "        fake_loss_A = adv_loss(D_A(fake_A), target_tensor(fake_A, False))\n",
    "        (real_loss_A + fake_loss_A).mul_(0.5).backward(); opt_D_A.step()\n",
    "\n",
    "        # DiscriminatorB (MRI)\n",
    "        opt_D_B.zero_grad()\n",
    "        real_loss_B = adv_loss(D_B(real_MRI), target_tensor(real_MRI, True))\n",
    "        fake_B      = fake_B_buffer.append(fake_MRI.detach()) or fake_MRI.detach()\n",
    "        fake_loss_B = adv_loss(D_B(fake_B), target_tensor(fake_B, False))\n",
    "        (real_loss_B + fake_loss_B).mul_(0.5).backward(); opt_D_B.step()\n",
    "\n",
    "    losses['adv'].append(sum_adv / len(train_loader))\n",
    "    losses['cycle'].append(sum_cycle / len(train_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch}/{total_epochs} | Adv: {losses['adv'][-1]:.4f} | Cycle: {losses['cycle'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e020491",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "Save model weight after every 100 epoch for reproducibility and recoverability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch % 100 == 0:\n",
    "        torch.save(G_BA.state_dict(), os.path.join(out_dir, f\"G_BA_ep{epoch}.pth\"))\n",
    "        torch.save(G_AB.state_dict(), os.path.join(out_dir, f\"G_AB_ep{epoch}.pth\"))\n",
    "        torch.save(D_A.state_dict(),  os.path.join(out_dir, f\"D_A_ep{epoch}.pth\"))\n",
    "        torch.save(D_B.state_dict(),  os.path.join(out_dir, f\"D_B_ep{epoch}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b892fb",
   "metadata": {},
   "source": [
    "# Visualization during Training\n",
    "Generate and save visual results with SSIM & PSNR metrics to monitor training quality and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027894f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch % 100 == 0:\n",
    "    with torch.no_grad():\n",
    "        real_CT, real_MRI = next(iter(train_loader))\n",
    "        real_CT, real_MRI = real_CT.to(device), real_MRI.to(device)\n",
    "        # MRI to CT to MRI\n",
    "        fake_CT = G_BA(real_MRI)\n",
    "        rec_MRI = G_AB(fake_CT)\n",
    "        # CT to MRI to CT\n",
    "        fake_MRI = G_AB(real_CT)\n",
    "        rec_CT   = G_BA(fake_MRI)\n",
    "    def denorm(x):\n",
    "        x = x[0].cpu().permute(1,2,0).numpy()\n",
    "        return x * 0.5 + 0.5\n",
    "\n",
    "    img_orig_MRI = denorm(real_MRI)\n",
    "    img_rec_MRI  = denorm(rec_MRI)\n",
    "    img_orig_CT  = denorm(real_CT)\n",
    "    img_rec_CT   = denorm(rec_CT)\n",
    "\n",
    "    ssim_MRI = ssim(img_orig_MRI, img_rec_MRI, multichannel=True)\n",
    "    psnr_MRI = psnr(img_orig_MRI, img_rec_MRI)\n",
    "    ssim_CT  = ssim(img_orig_CT, img_rec_CT, multichannel=True)\n",
    "    psnr_CT  = psnr(img_orig_CT, img_rec_CT)\n",
    "\n",
    "    # Visualize Output\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axs[0,0].imshow(img_orig_MRI); axs[0,0].set_title(\"Original MRI\")\n",
    "    axs[0,1].imshow(denorm(fake_CT));  axs[0,1].set_title(\"Generated CT\")\n",
    "    axs[0,2].imshow(img_rec_MRI);  axs[0,2].set_title(f\"Reconstructed MRI\\nSSIM {ssim_MRI:.3f}, PSNR {psnr_MRI:.1f}dB\")\n",
    "\n",
    "    axs[1,0].imshow(img_orig_CT); axs[1,0].set_title(\"Original CT\")\n",
    "    axs[1,1].imshow(denorm(fake_MRI)); axs[1,1].set_title(\"Generated MRI\")\n",
    "    axs[1,2].imshow(img_rec_CT);  axs[1,2].set_title(f\"Reconstructed CT\\nSSIM {ssim_CT:.3f}, PSNR {psnr_CT:.1f}dB\")\n",
    " \n",
    "    plt.suptitle(f\"Epoch {epoch} Cycle-Consistency Check\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"train_cycle_vis_ep{epoch}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9954",
   "metadata": {},
   "source": [
    "# Convergence\n",
    "Plot and save training loss curve to visualize CycleGAN convergence over epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1,total_epochs+1), losses['adv'], label='Adversarial Loss')\n",
    "plt.plot(range(1,total_epochs+1), losses['cycle'], label='Cycle Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "plt.title('Training Convergence')\n",
    "plt.savefig(os.path.join(out_dir, \"convergence.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c278a01",
   "metadata": {},
   "source": [
    "## 6. Testing\n",
    "Generate qualitative and quantitative result on random 10 test samples for both \"MRI to CT\" and \"CT to MRI\" translation. It visualizes inputs, predictions, and reconstruction, and calculates average PSNR, SSIM, and LPIPS to assess image quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = random.sample(range(len(test_ds)), 10)\n",
    "fig1, axs1 = plt.subplots(10,3, figsize=(9,30))\n",
    "fig2, axs2 = plt.subplots(10,3, figsize=(9,30))\n",
    "metrics_BA, metrics_AB = [], []\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    real_CT, real_MRI = test_ds[idx]\n",
    "    real_CT  = real_CT.unsqueeze(0).to(device)\n",
    "    real_MRI = real_MRI.unsqueeze(0).to(device)\n",
    "\n",
    "    # MRI → CT → MRI cycle\n",
    "    fake_CT = G_BA(real_MRI)\n",
    "    rec_MRI = G_AB(fake_CT)\n",
    "    orig   = real_MRI[0].cpu().permute(1,2,0).numpy()*0.5+0.5\n",
    "    recon  = rec_MRI[0].cpu().permute(1,2,0).numpy()*0.5+0.5\n",
    "    p, s = psnr(orig, recon), ssim(orig, recon, multichannel=True)\n",
    "    l = lpips_fn(real_MRI, rec_MRI).item()\n",
    "    metrics_BA.append((p,s,l))\n",
    "    axs1[i,0].imshow(orig); axs1[i,0].set_title(\"Original MRI\")\n",
    "    axs1[i,1].imshow((fake_CT[0].cpu().permute(1,2,0).numpy()*0.5+0.5)); axs1[i,1].set_title(\"Generated CT\")\n",
    "    axs1[i,2].imshow(recon); axs1[i,2].set_title(f\"Reconstructed MRI\\nPSNR{s:.1f},SSIM{s:.3f}\")\n",
    "\n",
    "    # CT → MRI → CT cycle\n",
    "    fake_MRI = G_AB(real_CT)\n",
    "    rec_CT   = G_BA(fake_MRI)\n",
    "    im_orig2  = real_CT[0].cpu().permute(1,2,0).numpy()*0.5+0.5\n",
    "    im_rec2 = rec_CT[0].cpu().permute(1,2,0).numpy()*0.5+0.5\n",
    "    p2, s2 = psnr(im_orig2, im_rec2), ssim(im_orig2, im_rec2, multichannel=True)\n",
    "    l2 = lpips_fn(real_CT, rec_CT).item()\n",
    "    metrics_AB.append((p2,s2,l2))\n",
    "    axs2[i,0].imshow(im_orig2); axs2[i,0].set_title(\"Original CT\")\n",
    "    axs2[i,1].imshow((fake_MRI[0].cpu().permute(1,2,0).numpy()*0.5+0.5)); axs2[i,1].set_title(\"Generated MRI\")\n",
    "    axs2[i,2].imshow(im_rec2); axs2[i,2].set_title(f\"Reconstructed CT\\nPSNR{p2:.1f},SSIM{s2:.3f}\")\n",
    "\n",
    "# Save figures\n",
    "fig1.tight_layout(); fig1.savefig(os.path.join(out_dir, \"test_MRI2CT.png\"))\n",
    "fig2.tight_layout(); fig2.savefig(os.path.join(out_dir, \"test_CT2MRI.png\"))\n",
    "plt.close('all')\n",
    "\n",
    "# Print average cycle‐consistency metrics\n",
    "print(\"Test MRI→CT avg PSNR,SSIM,LPIPS:\", np.mean(metrics_BA,0))\n",
    "print(\"Test CT→MRI avg PSNR,SSIM,LPIPS:\", np.mean(metrics_AB,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
